{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer with only encoder and decoder block that can translate English text to Arabic, given one file that contains English and its translated Arabic text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGQPvngvrnJ0"
   },
   "source": [
    "# Importing Required Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNU0PDoA7Kak"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWlfqjG2ruiN"
   },
   "source": [
    "# Reading Data and converting it to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCQ7ouWj75qf"
   },
   "outputs": [],
   "source": [
    "English = []\n",
    "Arabic = []\n",
    "with open('/content/ara_eng.txt', 'r',encoding='utf-8') as file:\n",
    "     lines = file.read().split(\"\\n\")[:-1]\n",
    "     for line in lines:\n",
    "       English.append(line.split(\"\\t\")[0])\n",
    "       Arabic.append(line.split(\"\\t\")[1])\n",
    "\n",
    "data = {\"English\": English, \"Arabic\": Arabic}\n",
    "df = pd.DataFrame(data)\n",
    "df = df.iloc[0:10000]\n",
    "df[\"Arabic\"] = \"<start>\" + df[\"Arabic\"] + \"<end>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKOUXc2Kr162"
   },
   "source": [
    "# Maximum Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHRkOuanOCW8"
   },
   "outputs": [],
   "source": [
    "Sequence_Length = 0\n",
    "for sequence in df['Arabic']:\n",
    "    sequence = sequence.split()\n",
    "    if  len(sequence) > Sequence_Length:\n",
    "        Sequence_Length = len(sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KC0HgSnpr9zj"
   },
   "source": [
    "# Vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XddktyCEO9BV"
   },
   "outputs": [],
   "source": [
    "text_data = \" \".join(df['English'].values.tolist())\n",
    "English_Vocab_Size = text_data.split()\n",
    "English_Vocab_Size = len(set(English_Vocab_Size))\n",
    "\n",
    "text_data = \" \".join(df['Arabic'].values.tolist())\n",
    "Arabic_Vocab_Size = text_data.split()\n",
    "Arabic_Vocab_Size = len(set(Arabic_Vocab_Size))\n",
    "Vocab_Size = English_Vocab_Size + Arabic_Vocab_Size\n",
    "\n",
    "# This code takes the values in the 'English' column of the DataFrame df and joins them into a single string, text_data. \n",
    "# Then, the string is split into individual words using the .split() method. \n",
    "# By converting the resulting list into a set and calculating its length, you obtain the number of unique words in the English language present in the DataFrame. \n",
    "# This value is stored in the English_Vocab_Size variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdvMkwQdsSFo"
   },
   "source": [
    "#  Text Vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBiS70CzKm_G"
   },
   "outputs": [],
   "source": [
    "# These layers are used to convert text into sequences of integers, which can be fed into the neural network models.\n",
    "\n",
    "English_Vectorization = TextVectorization(max_tokens=Vocab_Size, output_mode=\"int\", output_sequence_length=Sequence_Length,)\n",
    "\n",
    "Arabic_Vectorization = TextVectorization(max_tokens=Vocab_Size,output_mode=\"int\",output_sequence_length=Sequence_Length + 1,)\n",
    "# The additional +1 is likely because the target sequences were mentioned to be shifted by one timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgkm9PVssXZS"
   },
   "source": [
    "# Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPAQ5WCqVCBA"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['English'],df['Arabic'] , test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeHvlwTismcN"
   },
   "source": [
    "# Dataset Preparation and Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_GVNZtHVAFK"
   },
   "outputs": [],
   "source": [
    "English_Text = [text for text in X_train]\n",
    "Arabic_Text = [text for text in y_train]\n",
    "# Each element in the list represents a text sample.\n",
    "\n",
    "English_Vectorization.adapt(English_Text)\n",
    "Arabic_Vectorization.adapt(Arabic_Text)\n",
    "# This step analyzes the text data and builds the vocabulary based on the maximum number of tokens specified during the initialization of the vectorization layers. \n",
    "#It also sets up the internal mapping between words and integer indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAxTwhVUWSZ7"
   },
   "outputs": [],
   "source": [
    "def format_dataset(English, Arabic):\n",
    "    English = English_Vectorization(English)  \n",
    "    Arabic = Arabic_Vectorization(Arabic)  \n",
    "    return ({\"encoder_inputs\": English, \"decoder_inputs\": Arabic[:, :-1]}, Arabic[:, 1:])\n",
    "# The function returns a tuple containing the preprocessed input and target data for the model.\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((English_Text, Arabic_Text))  #This line creates a TensorFlow dataset, It combines the English and Arabic text data into a tuple of tensors, where each element of the tuple represents a sample pair.\n",
    "batch_size = 128  # used to group the individual samples into batches of size 128\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.map(format_dataset)  # This function converts the text sequences into integer sequences using the previously adapted TextVectorization layers.\n",
    "\n",
    "train = dataset.take(len(X_train))  # The take() method is used to create a new dataset containing the first len(X_train) samples\n",
    "test = dataset.skip(len(X_train))   # The skip() method is then used to create a new dataset containing the remaining samples, representing the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CalVHXCwsxxE"
   },
   "source": [
    "# Building a Transformer-based Neural Machine Translation Model with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2tVC4FXZ3yU"
   },
   "outputs": [],
   "source": [
    "# The code snippet you provided defines three custom layers for the Transformer model: TransformerEncoder, PositionalEmbedding, and TransformerDecoder. \n",
    "# These layers are building blocks of the Transformer architecture used in sequence-to-sequence tasks like machine translation.\n",
    "# Each layer defines its own call() method, which defines the forward pass logic, and some layers define a compute_mask() method to handle masking. \n",
    "# These layers can be used to construct the Transformer model for sequence-to-sequence tasks.\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "   # applies multi-head self-attention mechanism, projects the attention output, and adds residual connections and layer normalization. It supports masking to handle padded sequences.\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "    \n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "   # It consists of two embedding layers: one for token embeddings and another for positional embeddings. \n",
    "   # It combines the embeddings by adding them element-wise. The positional embeddings represent the position of each token in the sequence.\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "    # applies multi-head self-attention mechanism and encoder-decoder attention mechanism, projects the attention output, and adds residual connections and layer normalization. \n",
    "    # It also supports masking, including a causal attention mask for the self-attention mechanism to attend only to previous positions.\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult) # This line tiles (replicates) the mask tensor batch_size times along the batch axis using the multiplication tensor 'mult'  and represents the causal attention mask.\n",
    "    # this function is used within the call method of the TransformerDecoder class to apply the causal attention mask during the self-attention mechanism.\n",
    "    # generates a causal attention mask for the self-attention mechanism in the decoder. \n",
    "    # This mask ensures that each position in the decoder attends only to the previous positions, preventing information flow from future positions during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KxqYEYPIgs3W"
   },
   "outputs": [],
   "source": [
    "#  It defines separate models for the encoder and decoder and then combines them into a single Transformer model.\n",
    "\n",
    "embed_dim = 256  # The dimensionality of the token embeddings and positional embeddings.\n",
    "latent_dim = 1024  # The dimensionality of the intermediate dense layer in the TransformerDecoder.\n",
    "num_heads = 4  # The number of attention heads in the multi-head attention layers.\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = PositionalEmbedding(Sequence_Length, Vocab_Size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(Sequence_Length, Vocab_Size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "x = layers.Dropout(0.1)(x)  # Apply dropout regularization to the decoder outputs\n",
    "decoder_outputs = layers.Dense(Vocab_Size, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7dpySZrUg8OP",
    "outputId": "a4131fc9-bda2-454a-e003-184c4af6eac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding (Position  (None, None, 256)   4888832     ['encoder_inputs[0][0]']         \n",
      " alEmbedding)                                                                                     \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, None, 256)   1578496     ['positional_embedding[0][0]']   \n",
      " erEncoder)                                                                                       \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, None, 19083)  12424075    ['decoder_inputs[0][0]',         \n",
      "                                                                  'transformer_encoder[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 18,891,403\n",
      "Trainable params: 18,891,403\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "11/71 [===>..........................] - ETA: 3:27 - loss: 8.4247 - accuracy: 0.2280"
     ]
    }
   ],
   "source": [
    "epochs = 30  \n",
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.fit(train, epochs=epochs, validation_data=test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
