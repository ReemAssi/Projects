import os
import math
import random
import numpy as np
import pandas as pd
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
import torchvision.transforms as transforms
from torchvision import transforms as T
import torchvision.transforms.functional as TF
from sklearn.metrics import accuracy_score, classification_report
from einops import rearrange
from einops.layers.torch import Rearrange
import torch.nn.functional as F
from torch.autograd import Variable
import copy
import torch
from torch import nn

seed = 0
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.cuda.empty_cache()

# Function to load data from directories
def data(path):
    class_labels = sorted(os.listdir(path))

    images = []
    labels = []

    for label, class_label in enumerate(class_labels):
        class_path = os.path.join(path, class_label)

        for image_name in os.listdir(class_path):
            images.append(image_name)
            labels.append(label)

    images = np.array(images)
    labels = np.array(labels)

    df = pd.DataFrame({'image_id': images , 'label': labels})
    df = df.sample(frac=1).reset_index(drop=True)  # Shuffle the dataframe
    return df

# Custom dataset class
class CustomDataset(Dataset):
    def __init__(self,path, df, transform=None):
        self.df = df
        self.transform = transform
        self.path = path

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        img_name = os.path.join(self.path, Cases[self.df.iloc[idx, 1]], self.df.iloc[idx, 0])
        image = Image.open(img_name).convert("RGB")  # Load image
        if self.transform:
            image = self.transform(image)
        label = self.df.iloc[idx, 1]
        return image, label

# Function to generate positional embeddings for patches
def positional_embeddings(patches, temperature=10000, dtype=torch.float32):
    _, h, w, dim, device, dtype = *patches.shape, patches.device, patches.dtype

    y, x = torch.meshgrid(torch.arange(h, device=device), torch.arange(w, device=device), indexing='ij')
    assert (dim % 4) == 0, 'feature dimension must be multiple of 4 for sin cos embedding'
    omega = torch.arange(dim // 4, device=device) / (dim // 4 - 1)
    omega = 1. / (temperature ** omega)

    y = y.flatten()[:, None] * omega[None, :]
    x = x.flatten()[:, None] * omega[None, :]
    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim=1)
    return pe.type(dtype)

# Patch Dropout module
class PatchDropout(nn.Module):
    def __init__(self, prob):
        super().__init__()
        assert 0 <= prob < 1.
        self.prob = prob

    def forward(self, x):
        if not self.training or self.prob == 0.:
            return x

        b, n, _, device = *x.shape, x.device

        batch_indices = torch.arange(b, device=device)
        batch_indices = rearrange(batch_indices, '... -> ... 1')
        num_patches_keep = max(1, int(n * (1 - self.prob)))
        patch_indices_keep = torch.randn(b, n, device=device).topk(num_patches_keep, dim=-1).indices

        return x[batch_indices, patch_indices_keep]

# Feedforward module
class FeedForward(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.LayerNorm(dim),
            nn.Linear(dim, hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, dim),
        )

    def forward(self, x):
        return self.net(x)

# Attention module
class Attention(nn.Module):
    def __init__(self, dim, heads=8, dim_head=64):
        super().__init__()
        inner_dim = dim_head * heads
        self.heads = heads
        self.scale = dim_head ** -0.5
        self.norm = nn.LayerNorm(dim)

        self.attend = nn.Softmax(dim=-1)

        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)
        self.to_out = nn.Linear(inner_dim, dim, bias=False)

    def forward(self, x):
        x = self.norm(x)

        qkv = self.to_qkv(x).chunk(3, dim=-1)
        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)

        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale

        attn = self.attend(dots)

        out = torch.matmul(attn, v)
        out = rearrange(out, 'b h n d -> b n (h d)')
        return self.to_out(out)

# Transformer module
class Transformer(nn.Module):
    def __init__(self, dim, depth, heads, dim_head, mlp_dim):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                Attention(dim, heads=heads, dim_head=dim_head),
                FeedForward(dim, mlp_dim),
            ]))

    def forward(self, x):
        for  attn, ff in self.layers:
            x1 = self.norm(x)
            x = x + (attn(x1))
            x = x + ff(x)
        return x

# Vision Transformer (ViT) module
class ViT(nn.Module):
    def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3, dim_head=64, patch_dropout=0.5):
        super().__init__()
        image_height, image_width = image_size
        patch_height, patch_width = patch_size

        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'

        num_patches = (image_height // patch_height) * (image_width // patch_width)
        patch_dim = channels * patch_height * patch_width

        self.to_patch_embedding = nn.Sequential(
            Rearrange('b c (h p1) (w p2) -> b h w (p1 p2 c)', p1=patch_height, p2=patch_width),
            nn.LayerNorm(patch_dim),
            nn.Linear(patch_dim, dim),
            nn.LayerNorm(dim)
        )

        self.patch_dropout = PatchDropout(patch_dropout)

        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim)

        self.to_latent = nn.Identity()
        self.linear_head = nn.Linear(dim, num_classes)

    def forward(self, img):
        *_, h, w, dtype = *img.shape, img.dtype
        device = img.device

        # Step 1: Divide the input image into patches
        x = self.to_patch_embedding(img)

        # Step 2: Positional Embedding
        pe = positional_embeddings(x)
        x = rearrange(x, 'b ... d -> b (...) d') + pe
        x = self.patch_dropout(x)

        # Step 3: Encoder Layers (multi-head self-attention and feedforward neural networks)
        x = self.transformer(x)
        x = x.mean(dim=1)
        x = self.to_latent(x)
        return self.linear_head(x)

# Function to train the model
def train_model(model, train_loader, validation_loader, optimizer, num_epochs=30):
    best_val_loss = float('inf')
    for epoch in range(num_epochs):
        model.train()
        train_loss = 0.0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)

            loss.backward()
            optimizer.step()

            train_loss += loss.item() * images.size(0)

        train_loss = train_loss / len(train_loader.dataset)
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for images, labels in validation_loader:
                images, labels = images.to(device), labels.to(device)

                outputs = model(images)
                loss = criterion(outputs, labels)

                val_loss += loss.item() * images.size(0)

        val_loss = val_loss / len(validation_loader.dataset)

        print(f"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}")

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), 'best_model.pth')

    print("Training finished.")

# Function to evaluate the model
def evaluate_model(model, loader, data, device):
    predictions = []
    with torch.no_grad():
        for batch in loader:
            x = batch[0].to(device)  # Move the input tensor to the specified device
            pred = model(x)
            predictions.append(pred.argmax(dim=1).cpu().numpy())  # Append the predictions to a list

    # Combine predictions if there are multiple batches
    predictions = np.concatenate(predictions)

    true_labels = data["label"]

    # Calculate overall accuracy
    accuracy = accuracy_score(true_labels, predictions)
    print(f"Overall Accuracy: {accuracy}")

    # Calculate accuracy per class
    class_report = classification_report(true_labels, predictions, output_dict=True)
    for class_name, metrics in class_report.items():
        if class_name.isdigit():  # Skip non-class entries
            class_acc = metrics['recall']  # 'recall' is the metric for accuracy in classification report
            print(f" {Cases[int(class_name)]} Accuracy: {class_acc}")

# Focal Loss module
class FocalLoss(nn.Module):
    def __init__(self, gamma=0, alpha=None, size_average=True):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha
        if isinstance(alpha, (float, int)):
            self.alpha = torch.Tensor([alpha, 1-alpha])
        if isinstance(alpha, list):
            self.alpha = torch.Tensor(alpha)
        self.size_average = size_average

    def forward(self, input, target):
        if input.dim() > 2:
            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W
            input = input.transpose(1, 2)    # N,C,H*W => N,H*W,C
            input = input.contiguous().view(-1, input.size(2))   # N,H*W,C => N*H*W,C
        target = target.view(-1, 1)

        logpt = F.log_softmax(input)
        logpt = logpt.gather(1, target)
        logpt = logpt.view(-1)
        pt = Variable(logpt.data.exp())

        if self.alpha is not None:
            if self.alpha.type() != input.data.type():
                self.alpha = self.alpha.type_as(input.data)
            at = self.alpha.gather(0, target.data.view(-1))
            logpt = logpt * Variable(at)

        loss = -1 * (1-pt)**self.gamma * logpt
        if self.size_average:
            return loss.mean()
        else:
            return loss.sum()

# Function to apply image augmentation
def apply_augmentation(image: torch.Tensor) -> torch.Tensor:
    image = T.ColorJitter(0.8, 0.8, 0.8, 0.2)(image)
    image = T.RandomGrayscale(p=0.2)(image)
    image = T.RandomHorizontalFlip()(image)
    image = T.GaussianBlur((3, 3), (0.1, 0.2))(image)
    image = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)
    return image

# MLP module
class MLP(nn.Module):
    def __init__(self, dim, embedding_size=256, hidden_size=2048, batch_norm_mlp=False):
        super().__init__()
        norm = nn.BatchNorm1d(hidden_size) if batch_norm_mlp else nn.Identity()
        self.net = nn.Sequential(
            nn.Linear(dim, hidden_size),
            norm,
            nn.ReLU(inplace=True),
            nn.Linear(hidden_size, embedding_size)
        )

    def forward(self, x):
        return self.net(x)

# Additional Projection Head module
class AddProjHead(nn.Module):
    def __init__(self, model, in_features, hidden_size=4096,
                 embedding_size=256, batch_norm_mlp=True):
        super(AddProjHead, self).__init__()
        self.backbone = model
        self.in_features = in_features  # Store the input features for the projection head

        # Remove last layer from the ViT model
        self.backbone.linear_head = nn.Identity()
        # Add MLP projection head
        self.projection = MLP(in_features, embedding_size, hidden_size=hidden_size, batch_norm_mlp=batch_norm_mlp)

    def forward(self, x, return_embedding=False):
        embedding = self.backbone(x)
        if return_embedding:
            return embedding
        return self.projection(embedding)

# Loss function for BYOL
def loss_fn(x, y):
    # L2 normalization
    x = F.normalize(x, dim=-1, p=2)
    y = F.normalize(y, dim=-1, p=2)
    return 2 - 2 * (x * y).sum(dim=-1)

# Exponential Moving Average module
class EMA():
    def __init__(self, alpha):
        super().__init__()
        self.alpha = alpha

    def update_average(self, old, new):
        if old is None:
            return new
        return old * self.alpha + (1 - self.alpha) * new

# BYOL model
class BYOL(nn.Module):
    def __init__(
            self,
            net,
            batch_norm_mlp=True,
            projection_size=256,
            projection_hidden_size=2048,
            moving_average_decay=0.99,
            use_momentum=True):

        super().__init__()
        self.net = net
        self.in_features = net.linear_head.in_features
        self.student_model = AddProjHead(model=net, in_features=self.in_features,
                                         embedding_size=projection_size,
                                         hidden_size=projection_hidden_size,
                                         batch_norm_mlp=batch_norm_mlp)
        self.use_momentum = use_momentum
        self.teacher_model = self._get_teacher()
        self.target_ema_updater = EMA(moving_average_decay)
        self.student_predictor = MLP(self.in_features, projection_size, projection_size, projection_hidden_size)
        # self.augment_fn = augment_fn

    @torch.no_grad()
    def _get_teacher(self):
        return copy.deepcopy(self.student_model)

    @torch.no_grad()
    def update_moving_average(self):
        assert self.use_momentum, 'you do not need to update the moving average, since you have turned off momentum ' \
                                  'for the target encoder '
        assert self.teacher_model is not None, 'target encoder has not been created yet'

        for student_params, teacher_params in zip(self.student_model.parameters(), self.teacher_model.parameters()):
            old_weight, up_weight = teacher_params.data, student_params.data
            teacher_params.data = self.target_ema_updater.update_average(old_weight, up_weight)

    def forward(self, x, return_embedding=False):
        # Create two augmented views of the input image
        image_one, image_two = apply_augmentation(x), apply_augmentation(x)

        # compute the embeddings of both images using the student model.
        student_proj_one = self.student_model(image_one)
        student_proj_two = self.student_model(image_two)

        # The embeddings are then passed through a predictor MLP (student_predictor) to obtain the predictions.
        # Additional student's MLP head called predictor
        student_pred_one = self.student_predictor(student_proj_one)
        student_pred_two = self.student_predictor(student_proj_two)

        with torch.no_grad():
            teacher_proj_one = self.teacher_model(image_one).detach_()
            teacher_proj_two = self.teacher_model(image_two).detach_()

        loss_one = loss_fn(student_pred_one, teacher_proj_one)
        loss_two = loss_fn(student_pred_two, teacher_proj_two)

        return (loss_one + loss_two).mean()


if __name__ == '__main__':

    # Paths to dataset
    train_dataset_path = '/home/user/Derma/Combined_Train'
    test_dataset_path = '/home/user/Derma/Dataset_Splits/test'
    val_dataset_path = '/home/user/Derma/Dataset_Splits/val'

    # Load dataset
    train = data(train_dataset_path)
    test = data(test_dataset_path)
    validation = data(val_dataset_path)

    # Reset index
    train = train.reset_index(drop=True)
    validation = validation.reset_index(drop=True)
    test = test.reset_index(drop=True)

    # Dictionary for class labels
    class_labels = sorted(os.listdir(train_dataset_path))
    Cases = {}
    for i in range(len(class_labels)):
        Cases[i] = class_labels[i]

    # Define transformations
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),])

    # Create datasets
    train_dataset = CustomDataset(train_dataset_path, train, transform=transform)
    validation_dataset = CustomDataset(val_dataset_path, validation, transform=transform)
    test_dataset = CustomDataset(test_dataset_path, test, transform=transform)

    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=128)
    validation_loader = DataLoader(validation_dataset, batch_size=64)
    test_loader = DataLoader(test_dataset, batch_size=64)

    loader = DataLoader(train_dataset + validation_dataset + test_dataset, batch_size=256)

    # Define ViT model
    model = ViT(
        image_size=(224, 224),
        patch_size=(16, 16),
        num_classes=6,
        dim=512,
        depth=6,
        heads=12,
        mlp_dim=128,
        channels=3,
        dim_head=64,
        patch_dropout=0.7).to(device)

    # Define optimizer
    optimizer = optim.Adam(model.parameters(), lr=0.0001)
    criterion = FocalLoss(gamma=1)

    # Define BYOL model
    byol_model = BYOL(
        net=model,
        batch_norm_mlp=True,
        projection_size=512,
        projection_hidden_size=2048,
        moving_average_decay=0.99,
        use_momentum=True).to(device)

    byol_model.to(device)
    lr = 1e-5
    optimizer = torch.optim.Adam(byol_model.parameters(), lr=lr)

    # Training loop
    n_epoch = 30
    for epoch in range(n_epoch):
        byol_model.train()
        train_loss = 0.0
        num_batches = len(loader)

        for images, _ in loader:
            images = images.to(device)
            optimizer.zero_grad()
            loss = byol_model(images)
            loss.backward()
            optimizer.step()

            # EMA update
            byol_model.update_moving_average()

            train_loss += loss.item()

        # Calculate average training loss for the epoch
        train_loss = train_loss / num_batches

        # Print the training loss for the epoch
        print(f"Epoch {epoch + 1}/{n_epoch}, Loss: {train_loss:.4f}")

    # Load the state dictionary of the student model
    model.load_state_dict(byol_model.student_model.backbone.state_dict())

    # Add linear head to the model
    model.linear_head = nn.Sequential(
        nn.Linear(512, 256),
        nn.ReLU(),
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Linear(128, 6)
    )

    optimizer = optim.Adam(model.parameters(), lr=0.0001)
    criterion = FocalLoss(gamma=1)

    # Train the model
    train_model(model, train_loader, validation_loader, optimizer)

    # Evaluate the model
    evaluate_model(model, test_loader, test, device)
